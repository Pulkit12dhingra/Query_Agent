{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca55945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this code we will try to load the document into a seperate vector database\n",
    "# then we will apply ranking over the retrieved documents from all the vector databases\n",
    "# the retriever will be a multi vector database retriever with ranking capabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1efaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the documents from the data/data_information folder\n",
    "# create a vector database for each document\n",
    "# then create a multi vector database retriever with ranking capabilities\n",
    "# then create a langchain agent with the retriever\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# For ranking\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "925c08ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['order_items.txt', 'support_tickets.txt', 'states.txt', 'products.txt', 'addresses.txt', 'orders.txt', 'countries.txt', 'users.txt', 'payments.txt', 'user_profiles.txt'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In[2]: Load documents from folder\n",
    "\n",
    "DATA_DIR = Path(\"../data/data_information\")\n",
    "assert DATA_DIR.exists(), f\"{DATA_DIR} does not exist\"\n",
    "\n",
    "def load_single_file(path: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a single text-like file into LangChain Documents.\n",
    "    You can extend for PDFs, etc.\n",
    "    \"\"\"\n",
    "    loader = TextLoader(str(path), encoding=\"utf-8\")\n",
    "    docs = loader.load()\n",
    "    # Attach metadata to remember which file they came from\n",
    "    for d in docs:\n",
    "        d.metadata[\"source_file\"] = path.name\n",
    "    return docs\n",
    "\n",
    "all_docs_by_file = {}  # filename -> [Document, ...]\n",
    "for file_path in DATA_DIR.iterdir():\n",
    "    if file_path.is_file():\n",
    "        docs = load_single_file(file_path)\n",
    "        all_docs_by_file[file_path.name] = docs\n",
    "\n",
    "all_docs_by_file.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "166d9a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'order_items.txt': 1,\n",
       " 'support_tickets.txt': 1,\n",
       " 'states.txt': 1,\n",
       " 'products.txt': 1,\n",
       " 'addresses.txt': 1,\n",
       " 'orders.txt': 1,\n",
       " 'countries.txt': 1,\n",
       " 'users.txt': 1,\n",
       " 'payments.txt': 1,\n",
       " 'user_profiles.txt': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In[3]: Split documents into chunks per file\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunked_docs_by_file = {}  # filename -> [Document chunks]\n",
    "\n",
    "for filename, docs in all_docs_by_file.items():\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    # Ensure metadata keeps track of original file\n",
    "    for c in chunks:\n",
    "        c.metadata[\"source_file\"] = filename\n",
    "    chunked_docs_by_file[filename] = chunks\n",
    "\n",
    "{fn: len(chs) for fn, chs in chunked_docs_by_file.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03227048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'order_items.txt': <langchain_community.vectorstores.faiss.FAISS at 0x12ccc2ba0>,\n",
       " 'support_tickets.txt': <langchain_community.vectorstores.faiss.FAISS at 0x12cd38e10>,\n",
       " 'states.txt': <langchain_community.vectorstores.faiss.FAISS at 0x12cd38f50>,\n",
       " 'products.txt': <langchain_community.vectorstores.faiss.FAISS at 0x12cd43820>,\n",
       " 'addresses.txt': <langchain_community.vectorstores.faiss.FAISS at 0x12cd43e10>,\n",
       " 'orders.txt': <langchain_community.vectorstores.faiss.FAISS at 0x12fba7ad0>,\n",
       " 'countries.txt': <langchain_community.vectorstores.faiss.FAISS at 0x12046b9b0>,\n",
       " 'users.txt': <langchain_community.vectorstores.faiss.FAISS at 0x12046bdf0>,\n",
       " 'payments.txt': <langchain_community.vectorstores.faiss.FAISS at 0x12fbc1450>,\n",
       " 'user_profiles.txt': <langchain_community.vectorstores.faiss.FAISS at 0x12fc12750>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstores_by_file = {}  # filename -> FAISS vectorstore\n",
    "\n",
    "for filename, chunks in chunked_docs_by_file.items():\n",
    "    if not chunks:\n",
    "        continue\n",
    "    vs = FAISS.from_documents(chunks, embeddings)\n",
    "    vectorstores_by_file[filename] = vs\n",
    "\n",
    "vectorstores_by_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc6c586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[5]: Multi-vector retriever with pure embedding ranking\n",
    "\n",
    "def retrieve_from_all(\n",
    "    query: str,\n",
    "    k_per_store: int = 4,\n",
    ") -> List[Tuple[Document, float, str]]:\n",
    "    \"\"\"\n",
    "    Query each vectorstore and return (doc, distance, source_file).\n",
    "    distance: smaller = better match.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for filename, vs in vectorstores_by_file.items():\n",
    "        # For FAISS: similarity_search_with_score returns (doc, distance)\n",
    "        docs_and_scores = vs.similarity_search_with_score(query, k=k_per_store)\n",
    "        for doc, distance in docs_and_scores:\n",
    "            results.append((doc, distance, filename))\n",
    "    return results\n",
    "\n",
    "\n",
    "def rank_results_by_distance(\n",
    "    results: List[Tuple[Document, float, str]],\n",
    "    top_n: int = 5,\n",
    ") -> List[Tuple[Document, float, str]]:\n",
    "    \"\"\"\n",
    "    Convert distances to relevance scores and sort.\n",
    "    Relevance = 1 / (1 + distance). Larger = more relevant.\n",
    "    \"\"\"\n",
    "    scored = []\n",
    "    for doc, distance, filename in results:\n",
    "        relevance = 1.0 / (1.0 + distance)\n",
    "        scored.append((doc, relevance, filename))\n",
    "\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored[:top_n]\n",
    "\n",
    "\n",
    "def multi_vector_search(\n",
    "    query: str,\n",
    "    k_per_store: int = 4,\n",
    "    top_n: int = 5,\n",
    ") -> List[Tuple[Document, float, str]]:\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "    - retrieve from every vector DB\n",
    "    - rank globally by embedding relevance\n",
    "    \"\"\"\n",
    "    raw_results = retrieve_from_all(query, k_per_store=k_per_store)\n",
    "    ranked = rank_results_by_distance(raw_results, top_n=top_n)\n",
    "    return ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23a171ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[6]: Pretty-print ranked results\n",
    "\n",
    "def print_ranked_results(query: str, top_n: int = 5):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    results = multi_vector_search(query, top_n=top_n)\n",
    "\n",
    "    for idx, (doc, relevance, filename) in enumerate(results, start=1):\n",
    "        snippet = doc.page_content.replace(\"\\n\", \" \")\n",
    "        if len(snippet) > 300:\n",
    "            snippet = snippet[:300] + \"...\"\n",
    "        print(f\"\\nRank #{idx}\")\n",
    "        print(f\"  Source file: {filename}\")\n",
    "        print(f\"  Relevance score: {relevance:.4f}\")\n",
    "        print(f\"  Metadata: {doc.metadata}\")\n",
    "        print(f\"  Snippet: {snippet}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c5d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[10]: Define test cases (gold labels)\n",
    "\n",
    "from typing import Set, Dict, Any, List\n",
    "\n",
    "\"\"\"\n",
    "Each test case:\n",
    "- query: user query string\n",
    "- relevant_sources: set of source_file names considered relevant\n",
    "  (you decide this manually based on your knowledge of the files)\n",
    "\"\"\"\n",
    "\n",
    "test_cases: List[Dict[str, Any]] = [\n",
    "    {\n",
    "        \"name\": \"User demographics\",\n",
    "        \"query\": \"What data describes the demographics of a user?\",\n",
    "        \"relevant_sources\": {\"users.txt\", \"user_profiles.txt\", \"countries.txt\", \"states.txt\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Order lifecycle\",\n",
    "        \"query\": \"Explain the lifecycle of an order and its statuses.\",\n",
    "        \"relevant_sources\": {\"orders.txt\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Orders and payments\",\n",
    "        \"query\": \"How are orders linked to payments and payment status?\",\n",
    "        \"relevant_sources\": {\"orders.txt\", \"payments.txt\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Products and pricing\",\n",
    "        \"query\": \"What fields describe a product, its price and availability?\",\n",
    "        \"relevant_sources\": {\"products.txt\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Order line items\",\n",
    "        \"query\": \"Where can I find details for items inside an order?\",\n",
    "        \"relevant_sources\": {\"order_items.txt\", \"orders.txt\", \"products.txt\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Addresses and geography\",\n",
    "        \"query\": \"How is a user's address and location represented in the data model?\",\n",
    "        \"relevant_sources\": {\"addresses.txt\", \"countries.txt\", \"states.txt\", \"users.txt\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Customer support\",\n",
    "        \"query\": \"Which tables track customer support issues and their status?\",\n",
    "        \"relevant_sources\": {\"support_tickets.txt\", \"users.txt\", \"orders.txt\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Revenue and monetary values\",\n",
    "        \"query\": \"Which tables contain monetary amounts used for revenue analysis?\",\n",
    "        \"relevant_sources\": {\"orders.txt\", \"payments.txt\", \"order_items.txt\", \"products.txt\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Customer lifetime value\",\n",
    "        \"query\": \"What tables are needed to compute customer lifetime value?\",\n",
    "        \"relevant_sources\": {\"orders.txt\", \"payments.txt\", \"users.txt\", \"user_profiles.txt\"},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Shopping workflow\",\n",
    "        \"query\": \"Describe the workflow from browsing products to paying for an order.\",\n",
    "        \"relevant_sources\": {\"products.txt\", \"orders.txt\", \"order_items.txt\", \"payments.txt\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# In[11]: Helper to get ranked sources from your retriever\n",
    "\n",
    "def get_ranked_sources_for_query(query: str, top_n: int = 10) -> List[str]:\n",
    "    \"\"\"\n",
    "    Uses your multi_vector_search() which returns:\n",
    "    List[(Document, score, filename)]\n",
    "    We map to a ranked list of unique source filenames.\n",
    "    \"\"\"\n",
    "    results = multi_vector_search(query, top_n=top_n)\n",
    "    \n",
    "    ranked_sources: List[str] = []\n",
    "    seen: Set[str] = set()\n",
    "    for doc, score, filename in results:\n",
    "        sf = doc.metadata.get(\"source_file\", filename)\n",
    "        if sf not in seen:\n",
    "            ranked_sources.append(sf)\n",
    "            seen.add(sf)\n",
    "    return ranked_sources\n",
    "\n",
    "\n",
    "# For BaseRetriever:\n",
    "#docs = mv_retriever.get_relevant_documents(query)\n",
    "#ranked_sources = [d.metadata[\"source_file\"] for d in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf7bc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[12]: Metric functions\n",
    "\n",
    "import math\n",
    "\n",
    "def precision_at_k(pred: List[str], rel: Set[str], k: int) -> float:\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    pred_k = pred[:k]\n",
    "    if len(pred_k) == 0:\n",
    "        return 0.0\n",
    "    num_rel = sum(1 for p in pred_k if p in rel)\n",
    "    return num_rel / len(pred_k)\n",
    "\n",
    "\n",
    "def recall_at_k(pred: List[str], rel: Set[str], k: int) -> float:\n",
    "    if not rel:\n",
    "        return 0.0\n",
    "    pred_k = pred[:k]\n",
    "    num_rel = sum(1 for p in pred_k if p in rel)\n",
    "    return num_rel / len(rel)\n",
    "\n",
    "\n",
    "def average_precision(pred: List[str], rel: Set[str]) -> float:\n",
    "    \"\"\"\n",
    "    AP = average of precision@k at all ranks k where the item is relevant.\n",
    "    \"\"\"\n",
    "    if not rel:\n",
    "        return 0.0\n",
    "    ap_sum = 0.0\n",
    "    num_hits = 0\n",
    "    for i, p in enumerate(pred, start=1):\n",
    "        if p in rel:\n",
    "            num_hits += 1\n",
    "            ap_sum += num_hits / i\n",
    "    if num_hits == 0:\n",
    "        return 0.0\n",
    "    return ap_sum / len(rel)\n",
    "\n",
    "\n",
    "def reciprocal_rank(pred: List[str], rel: Set[str]) -> float:\n",
    "    \"\"\"\n",
    "    RR = 1 / rank of first relevant document, 0 if none.\n",
    "    \"\"\"\n",
    "    for i, p in enumerate(pred, start=1):\n",
    "        if p in rel:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def dcg_at_k(pred: List[str], rel: Set[str], k: int) -> float:\n",
    "    \"\"\"\n",
    "    DCG with binary relevance: rel_i âˆˆ {0,1}\n",
    "    \"\"\"\n",
    "    dcg = 0.0\n",
    "    for i, p in enumerate(pred[:k], start=1):\n",
    "        rel_i = 1.0 if p in rel else 0.0\n",
    "        if rel_i > 0:\n",
    "            dcg += rel_i / math.log2(i + 1)\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def ndcg_at_k(pred: List[str], rel: Set[str], k: int) -> float:\n",
    "    \"\"\"\n",
    "    nDCG@K = DCG@K / IDCG@K\n",
    "    With binary relevance, ideal ranking is all relevant docs at the top.\n",
    "    \"\"\"\n",
    "    if not rel:\n",
    "        return 0.0\n",
    "    dcg = dcg_at_k(pred, rel, k)\n",
    "    \n",
    "    # Ideal DCG: all relevant docs sorted first (but capped at k)\n",
    "    ideal_list = [1.0] * min(len(rel), k)\n",
    "    idcg = 0.0\n",
    "    for i, rel_i in enumerate(ideal_list, start=1):\n",
    "        idcg += rel_i / math.log2(i + 1)\n",
    "    if idcg == 0.0:\n",
    "        return 0.0\n",
    "    return dcg / idcg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f17f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[13]: Evaluate retriever on test cases\n",
    "\n",
    "def evaluate_retriever(\n",
    "    test_cases: List[Dict[str, Any]],\n",
    "    top_n: int = 10,\n",
    "    k_metrics: int = 5,  # for P@k, R@k, nDCG@k\n",
    "):\n",
    "    results = []\n",
    "    \n",
    "    for tc in test_cases:\n",
    "        name = tc[\"name\"]\n",
    "        query = tc[\"query\"]\n",
    "        relevant_sources: Set[str] = set(tc[\"relevant_sources\"])\n",
    "        \n",
    "        pred_sources = get_ranked_sources_for_query(query, top_n=top_n)\n",
    "        \n",
    "        p_at_k = precision_at_k(pred_sources, relevant_sources, k_metrics)\n",
    "        r_at_k = recall_at_k(pred_sources, relevant_sources, k_metrics)\n",
    "        ap = average_precision(pred_sources, relevant_sources)\n",
    "        rr = reciprocal_rank(pred_sources, relevant_sources)\n",
    "        ndcg = ndcg_at_k(pred_sources, relevant_sources, k_metrics)\n",
    "        \n",
    "        results.append({\n",
    "            \"name\": name,\n",
    "            \"query\": query,\n",
    "            \"P@{}\".format(k_metrics): p_at_k,\n",
    "            \"R@{}\".format(k_metrics): r_at_k,\n",
    "            \"AP\": ap,\n",
    "            \"RR\": rr,\n",
    "            \"nDCG@{}\".format(k_metrics): ndcg,\n",
    "            \"pred_sources\": pred_sources,\n",
    "            \"relevant_sources\": relevant_sources,\n",
    "        })\n",
    "    \n",
    "    # Macro averages\n",
    "    avg_p = sum(r[\"P@{}\".format(k_metrics)] for r in results) / len(results)\n",
    "    avg_r = sum(r[\"R@{}\".format(k_metrics)] for r in results) / len(results)\n",
    "    avg_ap = sum(r[\"AP\"] for r in results) / len(results)\n",
    "    avg_rr = sum(r[\"RR\"] for r in results) / len(results)\n",
    "    avg_ndcg = sum(r[\"nDCG@{}\".format(k_metrics)] for r in results) / len(results)\n",
    "    \n",
    "    summary = {\n",
    "        \"P@{}\".format(k_metrics): avg_p,\n",
    "        \"R@{}\".format(k_metrics): avg_r,\n",
    "        \"MAP\": avg_ap,\n",
    "        \"MRR\": avg_rr,\n",
    "        \"nDCG@{}\".format(k_metrics): avg_ndcg,\n",
    "    }\n",
    "    \n",
    "    return results, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4c30ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary Metrics ===\n",
      "P@5: 0.560\n",
      "R@5: 0.942\n",
      "MAP: 0.934\n",
      "MRR: 1.000\n",
      "nDCG@5: 0.944\n",
      "\n",
      "=== Per-Query Results ===\n",
      "\n",
      "Test case: User demographics\n",
      "Query: What data describes the demographics of a user?\n",
      "P@5: 0.800, R@5: 1.000, AP: 0.950, RR: 1.000, nDCG@5: 0.983\n",
      "Relevant: ['countries.txt', 'states.txt', 'user_profiles.txt', 'users.txt']\n",
      "Predicted: ['user_profiles.txt', 'users.txt', 'countries.txt', 'addresses.txt', 'states.txt', 'support_tickets.txt', 'products.txt', 'orders.txt', 'order_items.txt', 'payments.txt']\n",
      "\n",
      "Test case: Order lifecycle\n",
      "Query: Explain the lifecycle of an order and its statuses.\n",
      "P@5: 0.200, R@5: 1.000, AP: 1.000, RR: 1.000, nDCG@5: 1.000\n",
      "Relevant: ['orders.txt']\n",
      "Predicted: ['orders.txt', 'products.txt', 'order_items.txt', 'payments.txt', 'support_tickets.txt', 'addresses.txt', 'states.txt', 'user_profiles.txt', 'countries.txt', 'users.txt']\n",
      "\n",
      "Test case: Orders and payments\n",
      "Query: How are orders linked to payments and payment status?\n",
      "P@5: 0.400, R@5: 1.000, AP: 1.000, RR: 1.000, nDCG@5: 1.000\n",
      "Relevant: ['orders.txt', 'payments.txt']\n",
      "Predicted: ['payments.txt', 'orders.txt', 'order_items.txt', 'support_tickets.txt', 'products.txt', 'addresses.txt', 'user_profiles.txt', 'states.txt', 'users.txt', 'countries.txt']\n",
      "\n",
      "Test case: Products and pricing\n",
      "Query: What fields describe a product, its price and availability?\n",
      "P@5: 0.200, R@5: 1.000, AP: 1.000, RR: 1.000, nDCG@5: 1.000\n",
      "Relevant: ['products.txt']\n",
      "Predicted: ['products.txt', 'order_items.txt', 'user_profiles.txt', 'support_tickets.txt', 'addresses.txt', 'orders.txt', 'payments.txt', 'states.txt', 'countries.txt', 'users.txt']\n",
      "\n",
      "Test case: Order line items\n",
      "Query: Where can I find details for items inside an order?\n",
      "P@5: 0.600, R@5: 1.000, AP: 1.000, RR: 1.000, nDCG@5: 1.000\n",
      "Relevant: ['order_items.txt', 'orders.txt', 'products.txt']\n",
      "Predicted: ['order_items.txt', 'products.txt', 'orders.txt', 'payments.txt', 'addresses.txt', 'support_tickets.txt', 'user_profiles.txt', 'states.txt', 'countries.txt', 'users.txt']\n",
      "\n",
      "Test case: Addresses and geography\n",
      "Query: How is a user's address and location represented in the data model?\n",
      "P@5: 0.800, R@5: 1.000, AP: 1.000, RR: 1.000, nDCG@5: 1.000\n",
      "Relevant: ['addresses.txt', 'countries.txt', 'states.txt', 'users.txt']\n",
      "Predicted: ['addresses.txt', 'users.txt', 'countries.txt', 'states.txt', 'user_profiles.txt', 'orders.txt', 'products.txt', 'support_tickets.txt', 'payments.txt', 'order_items.txt']\n",
      "\n",
      "Test case: Customer support\n",
      "Query: Which tables track customer support issues and their status?\n",
      "P@5: 0.400, R@5: 0.667, AP: 0.625, RR: 1.000, nDCG@5: 0.671\n",
      "Relevant: ['orders.txt', 'support_tickets.txt', 'users.txt']\n",
      "Predicted: ['support_tickets.txt', 'payments.txt', 'user_profiles.txt', 'orders.txt', 'products.txt', 'addresses.txt', 'order_items.txt', 'users.txt', 'states.txt', 'countries.txt']\n",
      "\n",
      "Test case: Revenue and monetary values\n",
      "Query: Which tables contain monetary amounts used for revenue analysis?\n",
      "P@5: 0.800, R@5: 1.000, AP: 1.000, RR: 1.000, nDCG@5: 1.000\n",
      "Relevant: ['order_items.txt', 'orders.txt', 'payments.txt', 'products.txt']\n",
      "Predicted: ['orders.txt', 'products.txt', 'order_items.txt', 'payments.txt', 'user_profiles.txt', 'addresses.txt', 'support_tickets.txt', 'users.txt', 'states.txt', 'countries.txt']\n",
      "\n",
      "Test case: Customer lifetime value\n",
      "Query: What tables are needed to compute customer lifetime value?\n",
      "P@5: 0.600, R@5: 0.750, AP: 0.761, RR: 1.000, nDCG@5: 0.788\n",
      "Relevant: ['orders.txt', 'payments.txt', 'user_profiles.txt', 'users.txt']\n",
      "Predicted: ['user_profiles.txt', 'users.txt', 'products.txt', 'support_tickets.txt', 'orders.txt', 'order_items.txt', 'addresses.txt', 'countries.txt', 'payments.txt', 'states.txt']\n",
      "\n",
      "Test case: Shopping workflow\n",
      "Query: Describe the workflow from browsing products to paying for an order.\n",
      "P@5: 0.800, R@5: 1.000, AP: 1.000, RR: 1.000, nDCG@5: 1.000\n",
      "Relevant: ['order_items.txt', 'orders.txt', 'payments.txt', 'products.txt']\n",
      "Predicted: ['order_items.txt', 'products.txt', 'payments.txt', 'orders.txt', 'support_tickets.txt', 'addresses.txt', 'user_profiles.txt', 'users.txt', 'states.txt', 'countries.txt']\n"
     ]
    }
   ],
   "source": [
    "# In[14]: Run evaluation\n",
    "\n",
    "results, summary = evaluate_retriever(test_cases, top_n=10, k_metrics=5)\n",
    "\n",
    "print(\"=== Summary Metrics ===\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k}: {v:.3f}\")\n",
    "\n",
    "print(\"\\n=== Per-Query Results ===\")\n",
    "for r in results:\n",
    "    print(f\"\\nTest case: {r['name']}\")\n",
    "    print(f\"Query: {r['query']}\")\n",
    "    print(f\"P@5: {r['P@5']:.3f}, R@5: {r['R@5']:.3f}, AP: {r['AP']:.3f}, RR: {r['RR']:.3f}, nDCG@5: {r['nDCG@5']:.3f}\")\n",
    "    print(f\"Relevant: {sorted(r['relevant_sources'])}\")\n",
    "    print(f\"Predicted: {r['pred_sources']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3770083c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
